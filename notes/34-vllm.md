# Efficient Memory Management for Large Language Model Serving with PagedAttention

本次工作以 LLM 推理过程中的内存分布为出发点，优化 LLM 推理过程中的 KV Cache 的管理

- 模型参数占据 65%，并且是一个常数，通常难以进行优化
- KV Cache 占据 30%，因此对于管理推理过程中的内存分布至关重要

LLM 推理过程中的 KV Cache 具有以下特点：

- KV Cache 的大小随着推理过程发生变化
- 其生命周期和长度不确定，无法提前获知

因此现有的推理系统：

- 容易收到外部碎片和内部碎片的影响
  - 内部碎片：按照模型接受的最大请求长度申请，但是实际请求长度可能远小于申请长度
  - 外部碎片：即使能够知道请求所需要的长度，但仍然会由于外部碎片导致内存利用率不高
- 并且连续的内存空间导致无法很好地实现鉴权，难以进行内存共享

现有推理系统内存浪费的组成部分：

- 为了后续推理过程所预留的槽位
- 由于 over-provision 导致的内部碎片
- 内存管理导致的外部碎片

本次工作提出了 PagedAttention 的算法，将 llm 推理过程中的 KV Cache 进行分页：

- 解决了内部碎片和外部碎片的问题，同时可以以 block 为粒度进行内存共享
- 因此可以取得近似于 0 的 KV Cache 内存浪费
