# Boosting DNN Cold Inference on Devices

边缘环境的两大特征：严格的硬件资源约束、多租户易挥发的边缘环境

- 导致 DNN 模型（多个）无法长期驻存在内存当中，并进一步导致冷启动频繁发生

冷启动的发生有两种方式：

- 主动冷启动：通常发生在使用频率较低的应用，用后即回收旨在减少内存占用
  - 通常使用 weight sharing 的方式对所有的 DNN 模型进行打包减少内存占用
- 被动冷启动：在一些移动设备（智能手机）操作系统会激进地回收应用的占用资源

在 DNN 模型的冷启动中主要包含以下部分：

- weight reading (10~100ms)
- memory allocation (0.1~10ms)
- gpu preparation (option, 1000+ms)
- weight transformation (1000+ms)
- model execution (100~1000ms)

## Optimization Knobs

面向上述模型冷启动时延的三个主要部分 (weight reading, weight transformation, model execution) 可以对应到以下的优化方式：

- kernel selection
- bypassed weight-transformation: post-transformed weights caching
- pipelined inference: order of operator execution and core binding

kernel selection

- 在机器学习模型中，不同的 operator 存在多种实现方式 (kernel) 以适配不同的情况
- 不同的情况包括：kernel size、stride size、输入输出通道数量、不同的硬件情况需要不同的优化实现、由于代码迭代遗留的实现
- 目前实现方式的选择主要通过硬编码的方式进行决策，并且通常只考虑热推理的性能情况没有考虑到模型的冷启动问题

post-transformed weights caching

- 不同的 kernel 对 transformation 和 execution 两个环节的亲和性不同
- 通过缓存转换后的参数可以避免 CPU 的计算时间并降低内存访问密度
- 在权重加载和权重转换阶段，往往是磁盘 IO 和内存访问是速度瓶颈
- 本质上是一种磁盘 IO 和内存访问的权衡，对于访存密集的应用有较好的效果

order of operator execution and core binding

- DNN 中 layer by layer 的计算模型提供了更细粒度的计算优化空间
- 权重加载、权重转换、模型执行，每一层都可以面向上述的三个阶段进行流水线执行
  - operation: each stage of a kernel
- 并且异构的设备导致不同算子有各自的结合倾向性

本文工作面向上述的优化方式，进一步设计了 kernel 的调度算法：

- 保证了大核和小核之间、小核和小核之间的工作负载均衡
- workload stealing technique：动态适应硬件变化，保证负载均衡
- 引入 GPU 的情况下，将 GPU 视为大核，将 CPU 视为小核

面向冷启动优化所启动的模型可能和面向热推理的所对应的模型不同：

- 在完成冷启动的空闲时间中进一步启动热推理所需的不同的模型
- 如果尚未加载完成在下一个推理的空闲时间继续加载
