# Fast Distributed Inference Serving for Large Language Models

DNN 和 LLM 的特征对比：

- LLM 的推理任务具有特殊的自回归模式，导致推理任务的长度不确定
  - 每轮迭代可以在一个固定的时延内输出一个 token
  - 但完整的输出需要进行不确定轮数的多次迭代
  - DNN 的执行时延具有确定性和高度可预测性的特征
- 但是输出的第一个 token 的计算开销往往要大于后续生成的 token

FastServe: 面向大语言模型的分布式推理系统

- 主要解决的任务是决定是否执行当前任务，是否被其他任务抢占队列

LLM 推理任务的主要难点在于：

- 无法确定任务的执行时间（半信息不可知，已知输入长度无法确定输出长度）
- 中途被抢占任务需要保存中间结果，导致内存开销增大

## 主要工作内容

skip-join 多层反馈队列调度器，实现面向迭代层级的抢占方式

- 由于 LLM 半语义无关的特征无法估计任务的剩余时间，无法实现传统的 SRPT 调度算法
- LLM 中存在半语义无关的特征，根据输入长度信息将到达作业分配到合适长度的等待队列
- 任务可以通过降级和升级实现任务的灵活调度，同时避免饥饿现象

GPU 内存管理机制，解决多层反馈队列 (MLFQ) 所引入内存开销

- 为了减少任务的平均等待时间或提高 GPU 的利用率，需要增大 GPU 内存的使用（在大模型中这个问题尤为严重）
  - 推理作业暂停执行并降级，需要将中间结果保存在 GPU 内存中
  - 为了增加 LLM 的推理吞吐量，减少加速器中的空泡，需要将更多的作业放入 GPU 内存进行批处理
- 传统的解决方法要么延迟新到达的作业，要么将低优先级的作业杀死
  - 延迟新到达的作业会导致作业的延迟，并可能导致 MLFS 退化为 FCFS 导致队头阻塞的发生
  - 杀死低优先级的作业导致计算资源的浪费，并且可能导致死锁的发生
- 在 GPU 内存即将耗尽的时候，将其卸载到主机内存中
- 在即将唤醒作业的时候，重新将其状态加载到 GPU 内存中
- 依据作业执行时间和可能饥饿导致提前执行，预估下一次调度时间 (ENST, estimated next scheduled time)
